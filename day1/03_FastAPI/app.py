import os
import torch
from transformers import pipeline
import time
import traceback
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
import nest_asyncio
from pyngrok import ngrok
import logging

# --- è¨­å®š ---

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è¨­å®š ---
# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
DEFAULT_MODEL_NAME = "google/gemma-2-2b-jpn-it"
MODEL_NAME = os.environ.get("LLM_MODEL_NAME", DEFAULT_MODEL_NAME)
logger.info(f"ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å: {MODEL_NAME}")

# # ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š
# MODEL_NAME = "google/gemma-2-2b-jpn-it"  # ãŠå¥½ã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´å¯èƒ½ã§ã™
# print(f"ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š: {MODEL_NAME}")

# # --- ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¯ãƒ©ã‚¹ ---
# class Config:
#     def __init__(self, model_name=MODEL_NAME):
#         self.MODEL_NAME = model_name

# config = Config(MODEL_NAME)

# --- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾© ---
app = FastAPI(
    title="ãƒ­ãƒ¼ã‚«ãƒ«LLM APIã‚µãƒ¼ãƒ“ã‚¹",
    description="transformersãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®API",
    version="1.0.0"
)
# app.state ã«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®å±æ€§ã‚’åˆæœŸåŒ–
app.state.model_pipeline = None

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚’è¿½åŠ 
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
class Message(BaseModel):
    role: str
    content: str

# ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆ
class SimpleGenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 512
    do_sample: Optional[bool] = True
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class GenerationResponse(BaseModel):
    generated_text: str
    response_time: float

# --- ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ---
def load_model(model_name: str) -> Optional[Pipeline]:
    """æ¨è«–ç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠã®æ”¹å–„ (CUDA > MPS > CPU)
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available(): # MPS ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ 
            device = "mps"
            # MPS ä½¿ç”¨æ™‚ã®æ³¨æ„: bfloat16 ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆãŒã‚ã‚‹
            # å¿…è¦ã«å¿œã˜ã¦ torch_dtype ã‚’ torch.float16 ã‚„ None ã«å¤‰æ›´
            logger.info("MPSãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚torch_dtype=torch.bfloat16 ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        else:
            device = "cpu"
        logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        logger.info(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™...")
        start_load_time = time.time()

        pipe = pipeline(
            "text-generation",
            model=model_name,
            # MPSã®å ´åˆã€bfloat16ãŒã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆã¯ float16 ã‚’è©¦ã™
            model_kwargs={"torch_dtype": torch.bfloat16 if device != "mps" else torch.float16},
            device=device
        )
        end_load_time = time.time()
        logger.info(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸ (æ‰€è¦æ™‚é–“: {end_load_time - start_load_time:.2f}ç§’)")
        return pipe
    except ImportError as e:
        logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}. transformers, torch, accelerate ãªã©ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚", exc_info=True)
        return None
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True) # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚‚ãƒ­ã‚°ã«å‡ºåŠ›
        return None

# --- ä¾å­˜æ€§æ³¨å…¥ ---
# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ä¾å­˜æ€§æ³¨å…¥é–¢æ•°
async def get_model_pipeline(request: Request) -> Pipeline:
    if request.app.state.model_pipeline is None:
        logger.error("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã§ãã¾ã›ã‚“ã€‚")
        raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒåˆæœŸåŒ–ä¸­ã‹ã€ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    return request.app.state.model_pipeline

# å‹ãƒ’ãƒ³ãƒˆä»˜ãã®ä¾å­˜æ€§å®šç¾©
ModelPipelineDep = Annotated[Pipeline, Depends(get_model_pipeline)]

# --- FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã€app.state ã«æ ¼ç´"""
    logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ã‚¤ãƒ™ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
    pipeline_instance = load_model(MODEL_NAME)
    if pipeline_instance:
        app.state.model_pipeline = pipeline_instance
        logger.info("ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã€APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æº–å‚™ãŒã§ãã¾ã—ãŸã€‚")
    else:
        logger.error("èµ·å‹•æ™‚ã®ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã¯èµ·å‹•ã—ã¾ã™ãŒã€/generate ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚")

@app.get("/")
async def root():
    """åŸºæœ¬çš„ãªAPIãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "ok", "message": "Local LLM API is running"}

@app.get("/health")
async def health_check(request: Request):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    model_loaded = request.app.state.model_pipeline is not None
    status = "ok" if model_loaded else "error"
    message = "Model loaded successfully" if model_loaded else "Model not loaded or failed to load"
    return {"status": status, "message": message, "model_name": MODEL_NAME if model_loaded else None}

# ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (ä¾å­˜æ€§æ³¨å…¥ã‚’ä½¿ç”¨)
@app.post("/generate", response_model=GenerationResponse)
async def generate_simple(request: SimpleGenerationRequest, model_pipeline: ModelPipelineDep):
    """å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ (ä¾å­˜æ€§æ³¨å…¥ã‚’ä½¿ç”¨)"""
    try:
        start_time = time.time()
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã„å ´åˆã€ãƒ­ã‚°ã«ã¯ä¸€éƒ¨ã®ã¿è¡¨ç¤º
        log_prompt = request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt
        logger.info(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: prompt='{log_prompt}', max_new_tokens={request.max_new_tokens}")

        # ä¾å­˜æ€§æ³¨å…¥ã§å–å¾—ã—ãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨
        logger.info("ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
        outputs = model_pipeline(
            request.prompt,
            max_new_tokens=request.max_new_tokens,
            do_sample=request.do_sample,
            temperature=request.temperature,
            top_p=request.top_p,
            # Gemma-it ç”¨ã®åœæ­¢ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¿½åŠ  (ä»»æ„)
            # eos_token_id=model_pipeline.tokenizer.eos_token_id,
            # pad_token_id=model_pipeline.tokenizer.pad_token_id # pad_token_id ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        )
        logger.info("ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
        assistant_response = extract_assistant_response(outputs, request.prompt)
        log_response = assistant_response[:100] + "..." if len(assistant_response) > 100 else assistant_response
        logger.info(f"æŠ½å‡ºã•ã‚ŒãŸå¿œç­”: '{log_response}'")

        end_time = time.time()
        response_time = end_time - start_time
        logger.info(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.4f}ç§’")

        return GenerationResponse(
            generated_text=assistant_response,
            response_time=response_time
        )

    except Exception as e:
        logger.error(f"å¿œç­”ç”Ÿæˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¿œç­”ã®ç”Ÿæˆä¸­ã«å†…éƒ¨ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

# --- ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ---
# ãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# model = None

# def load_model():
#     """æ¨è«–ç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
#     global model  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«å¿…è¦
#     try:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
#         pipe = pipeline(
#             "text-generation",
#             model=config.MODEL_NAME,
#             model_kwargs={"torch_dtype": torch.bfloat16},
#             device=device
#         )
#         print(f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸ")
#         model = pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
#         return pipe
#     except Exception as e:
#         error_msg = f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}"
#         print(error_msg)
#         traceback.print_exc()  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
#         return None

def extract_assistant_response(outputs: List[Dict[str, Any]], user_prompt: str) -> str:
    """
    ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    Gemma-it ã®ä¸€èˆ¬çš„ãªå‡ºåŠ›å½¢å¼ã‚’æƒ³å®šã€‚
    """
    assistant_response = ""
    try:
        if not outputs or not isinstance(outputs, list) or len(outputs) == 0:
            logger.warning("ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å‡ºåŠ›ãŒç©ºã¾ãŸã¯äºˆæœŸã—ãªã„å½¢å¼ã§ã™ã€‚")
            return "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # é€šå¸¸ã€transformers pipeline ã¯ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ã«çµæœã‚’æ ¼ç´ã™ã‚‹
        generated_output = outputs[0].get("generated_text")

        if not generated_output or not isinstance(generated_output, str):
            logger.warning(f"å‡ºåŠ›ã‹ã‚‰ 'generated_text' ã‚’å–å¾—ã§ããªã„ã‹ã€æ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {outputs[0]}")
            return "å¿œç­”ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚"

        full_text = generated_output.strip()

        # Gemma Instructãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã¾ãšã«å¿œç­”ã®ã¿ã‚’è¿”ã™
        # ã‚‚ã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã€å‰Šé™¤ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹
        # ä¾‹: if full_text.startswith(user_prompt):
        #         assistant_response = full_text[len(user_prompt):].strip()
        #     else:
        #         assistant_response = full_text
        # ã“ã“ã§ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå«ã¾ã‚Œãªã„ã¨ä»®å®š
        assistant_response = full_text

        # ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šãƒãƒ£ãƒƒãƒˆå½¢å¼ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼‰ãŒå‡ºåŠ›ã«
        # <start_of_turn>model\n ã®ã‚ˆã†ãªãƒãƒ¼ã‚«ãƒ¼ã‚’å«ã‚€å ´åˆã€ãã‚Œã‚’å–ã‚Šé™¤ãå‡¦ç†ã‚’è¿½åŠ å¯èƒ½
        # model_turn_start = "<start_of_turn>model\n"
        # start_index = assistant_response.find(model_turn_start)
        # if start_index != -1:
        #     assistant_response = assistant_response[start_index + len(model_turn_start):].strip()

        # <end_of_turn> ãªã©ã®çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚Œã°å‰Šé™¤
        # end_turn = "<end_of_turn>"
        # if assistant_response.endswith(end_turn):
        #     assistant_response = assistant_response[:-len(end_turn)].strip()

    except Exception as e:
        logger.error(f"å¿œç­”ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        assistant_response = "å¿œç­”ã®æŠ½å‡ºå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    if not assistant_response:
        logger.warning(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›: {outputs}")
        assistant_response = "å¿œç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    return assistant_response

# def extract_assistant_response(outputs, user_prompt):
#     """ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã™ã‚‹"""
#     assistant_response = ""
#     try:
#         if outputs and isinstance(outputs, list) and len(outputs) > 0 and outputs[0].get("generated_text"):
#             generated_output = outputs[0]["generated_text"]
            
#             if isinstance(generated_output, list):
#                 # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆ
#                 if len(generated_output) > 0:
#                     last_message = generated_output[-1]
#                     if isinstance(last_message, dict) and last_message.get("role") == "assistant":
#                         assistant_response = last_message.get("content", "").strip()
#                     else:
#                         # äºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯æœ€å¾Œã®è¦ç´ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è©¦è¡Œ
#                         print(f"è­¦å‘Š: æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ãŒäºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã§ã™: {last_message}")
#                         assistant_response = str(last_message).strip()

#             elif isinstance(generated_output, str):
#                 # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
#                 full_text = generated_output
                
#                 # å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã®å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾Œã®å…¨ã¦ã‚’æŠ½å‡º
#                 if user_prompt:
#                     prompt_end_index = full_text.find(user_prompt)
#                     if prompt_end_index != -1:
#                         prompt_end_pos = prompt_end_index + len(user_prompt)
#                         assistant_response = full_text[prompt_end_pos:].strip()
#                     else:
#                         # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
#                         assistant_response = full_text
#                 else:
#                     assistant_response = full_text
#             else:
#                 print(f"è­¦å‘Š: äºˆæœŸã—ãªã„å‡ºåŠ›ã‚¿ã‚¤ãƒ—: {type(generated_output)}")
#                 assistant_response = str(generated_output).strip()  # æ–‡å­—åˆ—ã«å¤‰æ›

#     except Exception as e:
#         print(f"å¿œç­”ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
#         traceback.print_exc()
#         assistant_response = "å¿œç­”ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š

#     if not assistant_response:
#         print("è­¦å‘Š: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å®Œå…¨ãªå‡ºåŠ›:", outputs)
#         # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿”ã™
#         assistant_response = "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

#     return assistant_response

# --- FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
# @app.on_event("startup")
# async def startup_event():
#     """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
#     load_model_task()  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ãªãåŒæœŸçš„ã«èª­ã¿è¾¼ã‚€
#     if model is None:
#         print("è­¦å‘Š: èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
#     else:
#         print("èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# @app.get("/")
# async def root():
#     """åŸºæœ¬çš„ãªAPIãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
#     return {"status": "ok", "message": "Local LLM API is runnning"}

# @app.get("/health")
# async def health_check():
#     """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
#     global model
#     if model is None:
#         return {"status": "error", "message": "No model loaded"}

#     return {"status": "ok", "model": config.MODEL_NAME}

# ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# @app.post("/generate", response_model=GenerationResponse)
# async def generate_simple(request: SimpleGenerationRequest):
#     """å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
#     global model

#     if model is None:
#         print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™...")
#         load_model_task()  # å†åº¦èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
#         if model is None:
#             print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
#             raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

#     try:
#         start_time = time.time()
#         print(f"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: prompt={request.prompt[:100]}..., max_new_tokens={request.max_new_tokens}")  # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åˆ‡ã‚Šæ¨ã¦

#         # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ç›´æ¥å¿œç­”ã‚’ç”Ÿæˆ
#         print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
#         outputs = model(
#             request.prompt,
#             max_new_tokens=request.max_new_tokens,
#             do_sample=request.do_sample,
#             temperature=request.temperature,
#             top_p=request.top_p,
#         )
#         print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

#         # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
#         assistant_response = extract_assistant_response(outputs, request.prompt)
#         print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”: {assistant_response[:100]}...")  # é•·ã„å ´åˆã¯åˆ‡ã‚Šæ¨ã¦

#         end_time = time.time()
#         response_time = end_time - start_time
#         print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.2f}ç§’")

#         return GenerationResponse(
#             generated_text=assistant_response,
#             response_time=response_time
#         )

#     except Exception as e:
#         print(f"ã‚·ãƒ³ãƒ—ãƒ«å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
#         traceback.print_exc()
#         raise HTTPException(status_code=500, detail=f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# def load_model_task():
#     """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
#     global model
#     print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
#     # load_modelé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«è¨­å®š
#     loaded_pipe = load_model()
#     if loaded_pipe:
#         model = loaded_pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
#         print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
#     else:
#         print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# print("FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®šç¾©ã—ã¾ã—ãŸã€‚")

# --- ngrokã§APIã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•° ---
def run_with_ngrok(port=8501):
    """ngrokã§FastAPIã‚¢ãƒ—ãƒªã‚’å®Ÿè¡Œ"""
    nest_asyncio.apply() # Colab/Jupyterç’°å¢ƒã§asyncioã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒã‚¹ãƒˆå¯èƒ½ã«ã™ã‚‹

    ngrok_token = os.environ.get("NGROK_TOKEN")
    if not ngrok_token:
        logger.error("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒç’°å¢ƒå¤‰æ•° 'NGROK_TOKEN' ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        logger.info("Ngrokã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€https://dashboard.ngrok.com/get-started/your-authtoken ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã€")
        logger.info("ç’°å¢ƒå¤‰æ•° 'NGROK_TOKEN' ã«è¨­å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: Colab Secretsï¼‰ã€‚")
        # å¯¾è©±çš„ãªå…¥åŠ›ã¯ã‚µãƒ¼ãƒãƒ¼ç’°å¢ƒã§å•é¡Œã‚’èµ·ã“ã™ãŸã‚å‰Šé™¤
        # try:
        #     ngrok_token = input("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
        # except EOFError:
        #     logger.error("å¯¾è©±å‹å…¥åŠ›ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        #     return
        return # ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆã¯çµ‚äº†

    try:
        ngrok.set_auth_token(ngrok_token)
        logger.info("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")

        # æ—¢å­˜ã®ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹è©¦ã¿
        try:
            tunnels = ngrok.get_tunnels()
            if tunnels:
                logger.info(f"{len(tunnels)}å€‹ã®æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é–‰ã˜ã¦ã„ã¾ã™...")
                for tunnel in tunnels:
                    try:
                        ngrok.disconnect(tunnel.public_url)
                        logger.info(f"  - ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­: {tunnel.public_url}")
                    except Exception as disconnect_err:
                        logger.warning(f"  - ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼: {tunnel.public_url} - {disconnect_err}")
                logger.info("æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ã®åˆ‡æ–­å‡¦ç†ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                logger.info("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªngrokãƒˆãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            logger.warning(f"æ—¢å­˜ãƒˆãƒ³ãƒãƒ«ã®å–å¾—ãƒ»åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)

        # æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã
        logger.info(f"ãƒãƒ¼ãƒˆ {port} ã§æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š (é€šå¸¸ã¯ 'http')
        public_url = ngrok.connect(port, "http").public_url
        logger.info("---------------------------------------------------------------------")
        logger.info(f"âœ… FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸï¼")
        logger.info(f"âœ… å…¬é–‹URL (Public URL): {public_url}")
        logger.info(f"ğŸ“– APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Swagger UI): {public_url}/docs")
        logger.info(f"ğŸ©º ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: {public_url}/health")
        logger.info("---------------------------------------------------------------------")
        logger.info("(Ctrl+C ã¾ãŸã¯ ã‚»ãƒ«ã‚’åœæ­¢ ã™ã‚‹ã¨ã‚µãƒ¼ãƒãƒ¼ã¨ãƒˆãƒ³ãƒãƒ«ãŒçµ‚äº†ã—ã¾ã™)")

        # Uvicornã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•è¨­å®š
        # log_config=None ã§ uvicorn ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®šã‚’ç„¡åŠ¹åŒ–ã—ã€
        # Pythonæ¨™æº–ã®loggingè¨­å®šã«çµ±ä¸€ã™ã‚‹ï¼ˆä»»æ„ï¼‰
        uvicorn_config = uvicorn.Config(app, host="0.0.0.0", port=port, log_level="info", log_config=None)
        server = uvicorn.Server(uvicorn_config)

        # Uvicornã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œ (éåŒæœŸçš„ã«å®Ÿè¡Œã•ã‚Œã‚‹)
        server.run()
        # server.run() ãŒçµ‚äº†ã—ãŸã‚‰ (é€šå¸¸ã¯ Ctrl+C ãªã©ã§åœæ­¢ã•ã‚ŒãŸå ´åˆ)
        logger.info("Uvicornã‚µãƒ¼ãƒãƒ¼ãŒåœæ­¢ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        logger.error(f"ngrokã¾ãŸã¯Uvicornã®èµ·å‹•/å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
    finally:
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹
        try:
            logger.info("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¦ã„ã¾ã™...")
            ngrok.kill() # ã™ã¹ã¦ã®ngrokãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
            logger.info("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        except Exception as e:
            logger.warning(f"ngrokãƒˆãƒ³ãƒãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


# --- ngrokã§APIã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•° ---
# def run_with_ngrok(port=8501):
#     """ngrokã§FastAPIã‚¢ãƒ—ãƒªã‚’å®Ÿè¡Œ"""
#     nest_asyncio.apply()

#     ngrok_token = os.environ.get("NGROK_TOKEN")
#     if not ngrok_token:
#         print("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒ'NGROK_TOKEN'ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
#         try:
#             print("Colab Secrets(å·¦å´ã®éµã‚¢ã‚¤ã‚³ãƒ³)ã§'NGROK_TOKEN'ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
#             ngrok_token = input("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (https://dashboard.ngrok.com/get-started/your-authtoken): ")
#         except EOFError:
#             print("\nã‚¨ãƒ©ãƒ¼: å¯¾è©±å‹å…¥åŠ›ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
#             print("Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚»ãƒ«ã§`os.environ['NGROK_TOKEN'] = 'ã‚ãªãŸã®ãƒˆãƒ¼ã‚¯ãƒ³'`ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„")
#             return

#     if not ngrok_token:
#         print("ã‚¨ãƒ©ãƒ¼: Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸­æ­¢ã—ã¾ã™ã€‚")
#         return

#     try:
#         ngrok.set_auth_token(ngrok_token)

#         # æ—¢å­˜ã®ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹
#         try:
#             tunnels = ngrok.get_tunnels()
#             if tunnels:
#                 print(f"{len(tunnels)}å€‹ã®æ—¢å­˜ãƒˆãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é–‰ã˜ã¦ã„ã¾ã™...")
#                 for tunnel in tunnels:
#                     print(f"  - åˆ‡æ–­ä¸­: {tunnel.public_url}")
#                     ngrok.disconnect(tunnel.public_url)
#                 print("ã™ã¹ã¦ã®æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚")
#             else:
#                 print("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªngrokãƒˆãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
#         except Exception as e:
#             print(f"ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
#             # ã‚¨ãƒ©ãƒ¼ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšç¶šè¡Œã‚’è©¦ã¿ã‚‹

#         # æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã
#         print(f"ãƒãƒ¼ãƒˆ{port}ã«æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã„ã¦ã„ã¾ã™...")
#         ngrok_tunnel = ngrok.connect(port)
#         public_url = ngrok_tunnel.public_url
#         print("---------------------------------------------------------------------")
#         print(f"âœ… å…¬é–‹URL:   {public_url}")
#         print(f"ğŸ“– APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Swagger UI): {public_url}/docs")
#         print("---------------------------------------------------------------------")
#         print("(APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚„ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã“ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„)")
#         uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’infoã«è¨­å®š

#     except Exception as e:
#         print(f"\n ngrokã¾ãŸã¯Uvicornã®èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
#         traceback.print_exc()
#         # ã‚¨ãƒ©ãƒ¼å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚ˆã†ã¨ã™ã‚‹
#         try:
#             print("ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¦ã„ã¾ã™...")
#             tunnels = ngrok.get_tunnels()
#             for tunnel in tunnels:
#                 ngrok.disconnect(tunnel.public_url)
#             print("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
#         except Exception as ne:
#             print(f"ngrokãƒˆãƒ³ãƒãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«åˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ne}")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
if __name__ == "__main__":
    logger.info("ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯: FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚’é–‹å§‹ã—ã¾ã™...")
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    run_with_ngrok(port=8501) # ãƒãƒ¼ãƒˆç•ªå·ã¯å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
    logger.info("ã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

# # --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
# if __name__ == "__main__":
#     # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
#     run_with_ngrok(port=8501)  # ã“ã®ãƒãƒ¼ãƒˆç•ªå·ã‚’ç¢ºèª
#     # run_with_ngrokãŒçµ‚äº†ã—ãŸã¨ãã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
#     print("\nã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")