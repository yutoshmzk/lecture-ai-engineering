# app.py
import streamlit as st
import ui                   # UIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import llm                  # LLMãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import database             # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import metrics              # è©•ä¾¡æŒ‡æ¨™ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import data                 # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch
from transformers import pipeline
from config import MODEL_NAME
from huggingface_hub import HfFolder

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
st.set_page_config(page_title="Gemma Chatbot", layout="wide")

# --- åˆæœŸåŒ–å‡¦ç† ---
# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›èµ·å‹•æ™‚ãªã©ï¼‰
metrics.initialize_nltk()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ä½œæˆï¼‰
database.init_db()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
data.ensure_initial_data()

# LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰
# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨
@st.cache_resource
# def load_model():
#     """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
#     try:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         st.info(f"Using device: {device}") # ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º
#         pipe = pipeline(
#             "text-generation",
#             model=MODEL_NAME,
#             model_kwargs={"torch_dtype": torch.bfloat16},
#             device=device
#         )
#         st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
#         return pipe
#     except Exception as e:
#         st.error(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
#         st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¸è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
#         return None
# pipe = llm.load_model()

def load_model():
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠã®æ”¹å–„ (CUDA > MPS > CPU)
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"

        st.info(f"Using device: {device}") # ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ã®ã‚¹ãƒ”ãƒŠãƒ¼è¡¨ç¤º
        with st.spinner(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ ({device})..."):
            pipe = pipeline(
                "text-generation",
                model=MODEL_NAME,
                model_kwargs={"torch_dtype": torch.bfloat16},
                device=device,
                # token=HfFolder.get_token() # å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
            )
        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return pipe
    except ImportError as e:
        st.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        st.error("transformers, torch, accelerate ãªã©ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã€ãƒ¢ãƒ‡ãƒ«åã®èª¤ã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®å•é¡Œãªã©ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚")
        return None

# --- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ ---
# llm.load_model() ã§ã¯ãªãã€ä¸Šã§å®šç¾©ã—ãŸé–¢æ•°ã‚’å‘¼ã³å‡ºã™
pipe = load_model()

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.title("ğŸ¤– Gemma 2 Chatbot with Feedback")
st.write("Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å›ç­”ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œãˆã¾ã™ã€‚")
st.markdown("---")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä½¿ç”¨ã—ã¦é¸æŠãƒšãƒ¼ã‚¸ã‚’ä¿æŒ
if 'page' not in st.session_state:
    st.session_state.page = "ãƒãƒ£ãƒƒãƒˆ" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒšãƒ¼ã‚¸

page = st.sidebar.radio(
    "ãƒšãƒ¼ã‚¸é¸æŠ",
    ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"],
    key="page_selector",
    index=["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"].index(st.session_state.page), # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠçŠ¶æ…‹ã«ã™ã‚‹
    on_change=lambda: setattr(st.session_state, 'page', st.session_state.page_selector) # é¸æŠå¤‰æ›´æ™‚ã«çŠ¶æ…‹ã‚’æ›´æ–°
)


# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
if st.session_state.page == "ãƒãƒ£ãƒƒãƒˆ":
    if pipe:
        ui.display_chat_page(pipe)
    else:
        st.error("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
elif st.session_state.page == "å±¥æ­´é–²è¦§":
    ui.display_history_page()
elif st.session_state.page == "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
    ui.display_data_page()

# --- ãƒ•ãƒƒã‚¿ãƒ¼ãªã©ï¼ˆä»»æ„ï¼‰ ---
st.sidebar.markdown("---")
st.sidebar.info("é–‹ç™ºè€…: [Your Name]")